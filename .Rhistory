}
# Define the t values for which we want to modify alpha2
t_values <- c(1, 2, 4, 6)
# For each t, define alpha2 and beta2 and simulate the dataset
W1 <- simulate_Yij(alpha2 = c(t_values[1], -t_values[1], t_values[1], -t_values[1], t_values[1]),
beta2  = rep(1, 5), n = n, p = p)
alpha_1<-   rep(0, 5)
alpha_2<-  c(t_values[1], -t_values[1], t_values[1], -t_values[1], t_values[1])
s_jk_function <- function(cov_matrix) {
J <-ncol(cov_matrix)
diff <- matrix(0,J, J)  # Store sigma for each pair
for (j in 1:(J - 1)) {
for (k in (j + 1):J) {
# Compute covariance components
alpha_term <- (alpha_1[j] * (1 - p) + alpha_2[j] * p) * (alpha_1[k] * (1 - p) + alpha_2[k] * p)
#minus the alpha to get the in beta term
expected_cov <- alpha_1[k] * alpha_1[j] * (1 - p) + alpha_2[k] * alpha_2[j] * p  - alpha_term
diff[j,k] <-(cov_matrix[j, k] - expected_cov)
print(diff[j,k])
}
}
return (diff)
}
cov_matrix_W1 <- cov(W1)
s_matrix <-s_jk_function(cov_matrix_W1)
print(s_matrix)
params <- c(alpha0 = 0, gamma0 = 1, alpha1 = 2, gamma1 = 1.5, p = 0.3)
y <- c(rnorm(117, mean = params[1], sd = params[2]),
rnorm(50, mean = params[3], sd = params[4]))
#Generate the pdf marginal density
pdf <- function(y, alpha0, gamma0, alpha1, gamma1, p) {
term1 <- (1 - p) * (1 / (sqrt(2 * pi) * gamma0)) * exp(-((y - alpha0)^2) / (2 * gamma0^2))
term2 <- p * (1 / (sqrt(2 * pi) * gamma1)) * exp(-((y - alpha1)^2) / (2 * gamma1^2))
return(term1 + term2)
}
hist(y,breaks=20,prob=T,ylim=c(0,0.35))
y.grid <- seq(min(y),max(y),length.out=200)
y.pdf <- pdf(y.grid,alpha0=params[1],gamma0=params[2],
alpha1=params[3],gamma1=params[4],p=params[5])
lines(y.grid,y.pdf,type="l",lwd=2)
nllh <- function(params,y, p) {
alpha0 <- params[1]
gamma0 <- params[2]
alpha1 <- params[3]
gamma1 <- params[4]
#set.seed(1)  # For reproducibility
# Compute log-likelihood
log_likelihoods <- log(pdf(y, alpha0, gamma0, alpha1, gamma1,p))
return(-sum(log_likelihoods))
}
# Set initial guesses for parameters
initial_params <- c(alpha0 = 0, gamma0 = 1, alpha1 = 3, gamma1 = 1.5)
mle_result <- optim(
par = initial_params,
fn = nllh,
y=y, p = 0.3,
method = "BFGS"
)
mle_result$par
W1 <- simulate_Yij(alpha2 = c(t_values[1], -t_values[1], t_values[1], -t_values[1], t_values[1]),
beta2  = rep(1, 5), n = n, p = p)
alpha_1<-   rep(0, 5)
alpha_2<-  c(t_values[1], -t_values[1], t_values[1], -t_values[1], t_values[1])
s_jk_function <- function(cov_matrix) {
J <-ncol(cov_matrix)
diff <- matrix(0,J, J)  # Store sigma for each pair
for (j in 1:(J - 1)) {
for (k in (j + 1):J) {
# Compute covariance components
alpha_term <- (alpha_1[j] * (1 - p) + alpha_2[j] * p) * (alpha_1[k] * (1 - p) + alpha_2[k] * p)
#minus the alpha to get the in beta term
expected_cov <- alpha_1[k] * alpha_1[j] * (1 - p) + alpha_2[k] * alpha_2[j] * p  - alpha_term
diff[j,k] <-(cov_matrix[j, k] - expected_cov)
print(diff[j,k])
}
}
return (diff)
}
cov_matrix_W1 <- cov(W1)
s_matrix <-s_jk_function(cov_matrix_W1)
print(s_matrix)
pdf <- function(y, alpha0, gamma0, alpha1, gamma1, p) {
term1 <- (1 - p) * (1 / (sqrt(2 * pi) * gamma0)) * exp(-((y - alpha0)^2) / (2 * gamma0^2))
term2 <- p * (1 / (sqrt(2 * pi) * gamma1)) * exp(-((y - alpha1)^2) / (2 * gamma1^2))
return(term1 + term2)
}
estimate_gamma_per_variable <- function(y, p) {
J <- ncol(y)  # Number of variables
gamma0_j <- numeric(J)
gamma1_j <- numeric(J)
for (j in 1:J) {
# Extract column j
y_j <- y[, j]
# Define negative log-likelihood function
nllh_j <- function(params, y_j, p) {
alpha0 <- params[1]
gamma0 <- params[2]
alpha1 <- params[3]
gamma1 <- params[4]
log_likelihoods <- log(pdf(y_j, alpha0, gamma0, alpha1, gamma1, p))
return(-sum(log_likelihoods))
}
# Initial parameters
initial_params <- c(alpha0 = 0, gamma0 = 1, alpha1 = 3, gamma1 = 1.5)
# Run optimization
mle_result_j <- optim(
par = initial_params,
fn = nllh_j,
y_j = y_j,
p = p,
method = "BFGS"
)
# Store Gamma estimates
gamma0_j[j] <- mle_result_j$par[2]
gamma1_j[j] <- mle_result_j$par[4]
}
return(list(Gamma_0j = gamma0_j, Gamma_1j = gamma1_j))
}
gamma_estimates <- estimate_gamma_per_variable(y, p )
estimate_gamma_per_variable <- function(y, p) {
J <- ncol(y)  # Number of variables
gamma0_j <- numeric(J)
gamma1_j <- numeric(J)
for (j in 1:J) {
# Extract column j
y_j <- y[, j]
# Define negative log-likelihood function
nllh_j <- function(params, y_j, p) {
alpha0 <- params[1]
gamma0 <- params[2]
alpha1 <- params[3]
gamma1 <- params[4]
log_likelihoods <- log(pdf(y_j, alpha0, gamma0, alpha1, gamma1, p))
return(-sum(log_likelihoods))
}
# Initial parameters
initial_params <- c(alpha0 = 0, gamma0 = 1, alpha1 = 3, gamma1 = 1.5)
# Run optimization
mle_result_j <- optim(
par = initial_params,
fn = nllh_j,
y_j = y_j,
p = p,
method = "BFGS"
)
# Store Gamma estimates
gamma0_j[j] <- mle_result_j$par[2]
gamma1_j[j] <- mle_result_j$par[4]
}
return(list(Gamma_0j = gamma0_j, Gamma_1j = gamma1_j))
}
gamma_estimates <- estimate_gamma_per_variable(W1, p )
print(gamma_estimates)
# Function to solve for sigma
solve_sigma <- function(C_jk, gamma0, gamma1, p) {
f_sigma <- function(sigma) {
term1 <- (1 - p) * sqrt(gamma0^2 - sigma^2) * sqrt(gamma0^2 - sigma^2)
term2 <- p * sqrt(gamma1^2 - sigma^2) * sqrt(gamma1^2 - sigma^2)
return(C_jk - (term1 + term2))  # Solve for where this equals 0
}
# Ensure sigma stays within valid bounds
sigma_solution <- uniroot(f_sigma, interval = c(0, min(gamma0, gamma1) - 1e-6))$root
return(sigma_solution)
}
# Solve for sigma using estimated parameters
sigma_estimates <- solve_sigma(cov_matrix_W1[1,2], gamma0_hat, gamma1_hat, p_hat)
solve_sigma <- function(C_jk, Gamma_0j, Gamma_0k, Gamma_1j, Gamma_1k, p) {
f_sigma <- function(sigma) {
term1 <- (1 - p) * sqrt(Gamma_0j^2 - sigma^2) * sqrt(Gamma_0k^2 - sigma^2)
term2 <- p * sqrt(Gamma_1j^2 - sigma^2) * sqrt(Gamma_1k^2 - sigma^2)
return(C_jk - (term1 + term2))  # Solve for where this equals 0
}
# Ensure sigma stays within valid bounds
sigma_solution <- uniroot(f_sigma, interval = c(0, min(Gamma_0j, Gamma_0k, Gamma_1j, Gamma_1k) - 1e-6))$root
return(sigma_solution)
}
set.seed(123)
n <- 10000
p <- 0.2
# Helper functions to compute parameters based on a binary indicator
compute_alpha_j <- function(alpha1, alpha2, flag) {
alpha1 * (1 - flag) + alpha2 * flag
}
compute_beta_j <- function(beta1, beta2, flag) {
beta1 * (1 - flag) + beta2 * flag
}
# simulate_Yij accepts alpha2 and beta2 as inputs.
simulate_Yij <- function(alpha2, beta2, n, p) {
J <- length(alpha2)
Wij_matrix <- matrix(0, nrow = n, ncol = J)
# Baseline parameters: intercept 0 and slope 1
alpha1 <- rep(0, J)
beta1  <- rep(1, J)
# Standard deviations for error terms
sigma <- rep(0.5, J)
for(i in 1:n){
theta_i <- rnorm(1, mean = 0, sd = 1)
flag <- rbinom(1, size = 1, prob = p)
for(j in 1:J){
epsilon_ij <- rnorm(1, mean = 0, sd = sigma[j])
a_j <- compute_alpha_j(alpha1[j], alpha2[j], flag)
b_j <- compute_beta_j(beta1[j], beta2[j], flag)
Wij_matrix[i, j] <- a_j + b_j * theta_i + epsilon_ij
}
}
return(Wij_matrix)
}
# Define the t values for which we want to modify alpha2
t_values <- c(1, 2, 4, 6)
# For each t, define alpha2 and beta2 and simulate the dataset
W1 <- simulate_Yij(alpha2 = c(t_values[1], -t_values[1], t_values[1], -t_values[1], t_values[1]),
beta2  = rep(1, 5), n = n, p = p)
alpha_1<-   rep(0, 5)
alpha_2<-  c(t_values[1], -t_values[1], t_values[1], -t_values[1], t_values[1])
s_jk_function <- function(cov_matrix) {
J <-ncol(cov_matrix)
diff <- matrix(0,J, J)  # Store sigma for each pair
for (j in 1:(J - 1)) {
for (k in (j + 1):J) {
# Compute covariance components
alpha_term <- (alpha_1[j] * (1 - p) + alpha_2[j] * p) * (alpha_1[k] * (1 - p) + alpha_2[k] * p)
#minus the alpha to get the in beta term
expected_cov <- alpha_1[k] * alpha_1[j] * (1 - p) + alpha_2[k] * alpha_2[j] * p  - alpha_term
diff[j,k] <-(cov_matrix[j, k] - expected_cov)
print(diff[j,k])
}
}
return (diff)
}
cov_matrix_W1 <- cov(W1)
s_matrix <-s_jk_function(cov_matrix_W1)
print(s_matrix)
pdf <- function(y, alpha0, gamma0, alpha1, gamma1, p) {
term1 <- (1 - p) * (1 / (sqrt(2 * pi) * gamma0)) * exp(-((y - alpha0)^2) / (2 * gamma0^2))
term2 <- p * (1 / (sqrt(2 * pi) * gamma1)) * exp(-((y - alpha1)^2) / (2 * gamma1^2))
return(term1 + term2)
}
estimate_gamma_per_variable <- function(y, p) {
J <- ncol(y)  # Number of variables
gamma0_j <- numeric(J)
gamma1_j <- numeric(J)
for (j in 1:J) {
# Extract column j
y_j <- y[, j]
# Define negative log-likelihood function
nllh_j <- function(params, y_j, p) {
alpha0 <- params[1]
gamma0 <- params[2]
alpha1 <- params[3]
gamma1 <- params[4]
log_likelihoods <- log(pdf(y_j, alpha0, gamma0, alpha1, gamma1, p))
return(-sum(log_likelihoods))
}
# Initial parameters
initial_params <- c(alpha0 = 0, gamma0 = 1, alpha1 = 3, gamma1 = 1.5)
# Run optimization
mle_result_j <- optim(
par = initial_params,
fn = nllh_j,
y_j = y_j,
p = p,
method = "BFGS"
)
# Store Gamma estimates
gamma0_j[j] <- mle_result_j$par[2]
gamma1_j[j] <- mle_result_j$par[4]
}
return(list(Gamma_0j = gamma0_j, Gamma_1j = gamma1_j))
}
gamma_estimates <- estimate_gamma_per_variable(W1, p )
print(gamma_estimates)
solve_sigma <- function(C_jk, Gamma_0j, Gamma_0k, Gamma_1j, Gamma_1k, p) {
f_sigma <- function(sigma) {
term1 <- (1 - p) * sqrt(Gamma_0j^2 - sigma^2) * sqrt(Gamma_0k^2 - sigma^2)
term2 <- p * sqrt(Gamma_1j^2 - sigma^2) * sqrt(Gamma_1k^2 - sigma^2)
return(C_jk - (term1 + term2))  # Solve for where this equals 0
}
# Ensure sigma stays within valid bounds
sigma_solution <- uniroot(f_sigma, interval = c(0, min(Gamma_0j, Gamma_0k, Gamma_1j, Gamma_1k) - 1e-6))$root
return(sigma_solution)
}
# Solve for sigma for each (j, k) pair
solve_sigma_matrix <- function(s_matrix, gamma_estimates, p) {
J <- length(gamma_estimates$Gamma_0j)
sigma_matrix <- matrix(0, J, J)
for (j in 1:(J - 1)) {
for (k in (j + 1):J) {
sigma_matrix[j, k] <- solve_sigma(
C_jk = s_matrix[j, k],
Gamma_0j = gamma_estimates$Gamma_0j[j],
Gamma_0k = gamma_estimates$Gamma_0j[k],
Gamma_1j = gamma_estimates$Gamma_1j[j],
Gamma_1k = gamma_estimates$Gamma_1j[k],
p = p
)
sigma_matrix[k, j] <- sigma_matrix[j, k]  # Ensure symmetry
}
}
return(sigma_matrix)
}
# Compute sigma values
sigma_matrix <- solve_sigma_matrix(s_matrix, gamma_estimates, p)
# Print final estimated sigma values
print(sigma_matrix)
solve_sigma <- function(C_jk, Gamma_0j, Gamma_0k, Gamma_1j, Gamma_1k, p) {
f_sigma <- function(sigma) {
term1 <- (1 - p) * sqrt(Gamma_0j^2 - sigma^2) * sqrt(Gamma_0k^2 - sigma^2)
term2 <- p * sqrt(Gamma_1j^2 - sigma^2) * sqrt(Gamma_1k^2 - sigma^2)
return(C_jk - (term1 + term2))  # Solve for where this equals 0
}
# Ensure sigma stays within valid bounds
sigma_solution <- uniroot(f_sigma, interval = c(0, min(Gamma_0j, Gamma_0k, Gamma_1j, Gamma_1k) - 1e-6))$root
return(sigma_solution)
}
# Solve for sigma for each (j, k) pair
solve_sigma_matrix <- function(s_matrix, gamma_estimates, p) {
J <- length(gamma_estimates$Gamma_0j)
sigma_matrix <- matrix(0, J, J)
for (j in 1:(J - 1)) {
for (k in (j + 1):J) {
sigma_matrix[j, k] <- solve_sigma(
C_jk = s_matrix[j, k],
Gamma_0j = gamma_estimates$Gamma_0j[j],
Gamma_0k = gamma_estimates$Gamma_0j[k],
Gamma_1j = gamma_estimates$Gamma_1j[j],
Gamma_1k = gamma_estimates$Gamma_1j[k],
p = p
)
}
}
return(sigma_matrix)
}
# Compute sigma values
sigma_matrix <- solve_sigma_matrix(s_matrix, gamma_estimates, p)
# Print final estimated sigma values
print(sigma_matrix)
compute_sigma_i <- function(sigma_matrix) {
row_means <- rowMeans(sigma_matrix, na.rm = TRUE)  # Compute row-wise mean
return(row_means)
}
# Compute sigma_i values
sigma_i <- compute_sigma_i(sigma_matrix)
print(sigma_i)
# Importing the dataset
dataset = read.csv('Student_Performance.csv')
setwd("~/Data_Mining_Visualization/Group_Project")
# Importing the dataset
dataset = read.csv('Student_Performance.csv')
# Fitting SVR to the whole dataset
library(e1071)
library(caTools)
split <- sample.split(dataset$Y, SplitRatio = 0.75)
head(dataset)
split <- sample.split(dataset$  Performance.Index, SplitRatio = 0.75)
head(dataset)
# SVR
# Importing the dataset
dataset = read.csv('Student_Performance.csv')
head(dataset)
# Fitting SVR to the whole dataset
library(e1071)
library(caTools)
split <- sample.split(dataset$  Performance.Index, SplitRatio = 0.75)
training_set <- subset(dataset, split == TRUE)
test_set <- subset(dataset, split == FALSE)
regressor = svm(formula = Y ~ .,
data = training_set,
type = 'eps-regression',
kernel = 'radial')
# SVR
# Importing the dataset
dataset = read.csv('Student_Performance.csv')
# Fitting SVR to the whole dataset
library(e1071)
library(caTools)
dataset$smoker = factor(dataset$smoker,
levels = c('No', 'Yes'),
labels = c(0,1))
# Fitting SVR to the whole dataset
head(dataset)
# SVR
# Importing the dataset
dataset = read.csv('Student_Performance.csv')
# Fitting SVR to the whole dataset
head(dataset)
library(e1071)
library(caTools)
dataset$Extracurricular.Activities = factor(dataset$Extracurricular.Activities,
levels = c('No', 'Yes'),
labels = c(0,1))
split <- sample.split(dataset$  Performance.Index, SplitRatio = 0.75)
training_set <- subset(dataset, split == TRUE)
test_set <- subset(dataset, split == FALSE)
head(dataset)
# SVR
# Importing the dataset
dataset = read.csv('Student_Performance.csv')
# Fitting SVR to the whole dataset
head(dataset)
library(e1071)
library(caTools)
dataset$Extracurricular.Activities = factor(dataset$Extracurricular.Activities,
levels = c('No', 'Yes'),
labels = c(0,1))
split <- sample.split(dataset$  Performance.Index, SplitRatio = 0.75)
training_set <- subset(dataset, split == TRUE)
test_set <- subset(dataset, split == FALSE)
head(dataset)
regressor = svm(formula = Performance.Index ~ .,
data = training_set,
type = 'eps-regression',
kernel = 'radial')
# Making a  prediction
y_pred <-predict(regressor,newdata=test_set)
print(y_pred)
ssr = sum((test_set$Y - y_pred) ^ 2)
sst = sum((test_set$Y - mean(test_set$Y)) ^ 2)
r2 = 1 - (ssr/sst)
print(r2)
r2_adjusted = 1 - (1 - r2) * (length(test_set$Y) - 1) / (length(test_set$Y) - 6 - 1)
print(paste('adjusted svr', r2_adjusted))
# SVR
# Importing the dataset
dataset = read.csv('Student_Performance.csv')
# Fitting SVR to the whole dataset
head(dataset)
library(e1071)
library(caTools)
dataset$Extracurricular.Activities = factor(dataset$Extracurricular.Activities,
levels = c('No', 'Yes'),
labels = c(0,1))
split <- sample.split(dataset$  Performance.Index, SplitRatio = 0.75)
training_set <- subset(dataset, split == TRUE)
test_set <- subset(dataset, split == FALSE)
head(dataset)
regressor = svm(formula = Performance.Index ~ .,
data = training_set,
type = 'eps-regression',
kernel = 'radial')
# Making a  prediction
y_pred <-predict(regressor,newdata=test_set)
print(y_pred)
ssr = sum((test_set$Extracurricular.Activities - y_pred) ^ 2)
sst = sum((test_set$Extracurricular.Activities - mean(test_set$Extracurricular.Activities)) ^ 2)
r2 = 1 - (ssr/sst)
print(r2)
r2_adjusted = 1 - (1 - r2) * (length(test_set$Extracurricular.Activities) - 1) / (length(test_set$Extracurricular.Activities) - 6 - 1)
print(paste('adjusted svr', r2_adjusted))
ssr = sum((test_set$Performance.Index - y_pred) ^ 2)
sst = sum((test_set$Performance.Index - mean(test_set$Performance.Index)) ^ 2)
r2 = 1 - (ssr/sst)
print(r2)
r2_adjusted = 1 - (1 - r2) * (length(test_set$Performance.Index) - 1) / (length(test_set$Performance.Index) - 6 - 1)
print(paste('adjusted svr', r2_adjusted))
head(dataset)
print(ggplot() +
geom_point(aes(x = test_set$Previous.Scores, y = test_set$Performance.Index), colour = 'red') +
geom_point(aes(x = dataset$age, y = predict(regressor, newdata = dataset)), colour = 'blue') +
ggtitle('Age vs charge price ') +
xlab('age') + ylab('charges') +
xlim(x_range) + ylim(y_range))
library(ggplot2)
print(ggplot() +
geom_point(aes(x = test_set$Previous.Scores, y = test_set$Performance.Index), colour = 'red') +
geom_point(aes(x = test_set$Previous.Scores, y = predict(regressor, newdata = test_set)), colour = 'blue') +
ggtitle('Age vs charge price ') +
xlab('age') + ylab('charges') +
xlim(x_range) + ylim(y_range))
x_range <- range(dataset$Previous.Scores)
y_range <- range(dataset$Performance.Index)
print(ggplot() +
geom_point(aes(x = test_set$Previous.Scores, y = test_set$Performance.Index), colour = 'red') +
geom_point(aes(x = test_set$Previous.Scores, y = predict(regressor, newdata = test_set)), colour = 'blue') +
ggtitle('Age vs charge price ') +
xlab('age') + ylab('charges') +
xlim(x_range) + ylim(y_range))
# SVR
# Importing the dataset
dataset = read.csv('Student_Performance.csv')
# Fitting SVR to the whole dataset
head(dataset)
library(e1071)
library(caTools)
dataset$Extracurricular.Activities = factor(dataset$Extracurricular.Activities,
levels = c('No', 'Yes'),
labels = c(0,1))
split <- sample.split(dataset$  Performance.Index, SplitRatio = 0.75)
training_set <- subset(dataset, split == TRUE)
test_set <- subset(dataset, split == FALSE)
head(dataset)
regressor = svm(formula = Performance.Index ~ .,
data = training_set,
type = 'eps-regression',
kernel = 'radial')
# Making a  prediction
y_pred <-predict(regressor,newdata=test_set)
print(y_pred)
ssr = sum((test_set$Performance.Index - y_pred) ^ 2)
sst = sum((test_set$Performance.Index - mean(test_set$Performance.Index)) ^ 2)
r2 = 1 - (ssr/sst)
print(r2)
r2_adjusted = 1 - (1 - r2) * (length(test_set$Performance.Index) - 1) / (length(test_set$Performance.Index) - 6 - 1)
print(paste('adjusted svr', r2_adjusted))
library(ggplot2)
x_range <- range(dataset$Previous.Scores)
y_range <- range(dataset$Performance.Index)
print(ggplot() +
geom_point(aes(x = test_set$Previous.Scores, y = test_set$Performance.Index), colour = 'red') +
geom_point(aes(x = test_set$Previous.Scores, y = predict(regressor, newdata = test_set)), colour = 'blue') +
ggtitle('Previous score vs Current Performance Index ') +
xlab('Previous Score') + ylab('Current Performance Index') +
xlim(x_range) + ylim(y_range))
